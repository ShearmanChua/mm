version: '3.4'

volumes:
  elasticsearch-data:

secrets:
  elasticsearch.keystore:
    file: ../elastdocker/secrets/keystore/elasticsearch.keystore
  elasticsearch.service_tokens:
    file: ../elastdocker/secrets/service_tokens
  elastic.ca:
    file: ../elastdocker/secrets/certs/ca/ca.crt
  elasticsearch.certificate:
    file: ../elastdocker/secrets/certs/elasticsearch/elasticsearch.crt
  elasticsearch.key:
    file: ../elastdocker/secrets/certs/elasticsearch/elasticsearch.key
  kibana.certificate:
    file: ../elastdocker/secrets/certs/kibana/kibana.crt
  kibana.key:
    file: ../elastdocker/secrets/certs/kibana/kibana.key

services:
  elasticsearch:
    image: elastdocker/elasticsearch:${ELK_VERSION}
    restart: unless-stopped
    # env_file:
    # - ./.env
    environment:
      ELASTIC_USERNAME: ${ELASTIC_USERNAME}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTIC_CLUSTER_NAME: ${ELASTIC_CLUSTER_NAME}
      ELASTIC_NODE_NAME: ${ELASTIC_NODE_NAME}
      ELASTIC_INIT_MASTER_NODE: ${ELASTIC_INIT_MASTER_NODE}
      ELASTIC_DISCOVERY_SEEDS: ${ELASTIC_DISCOVERY_SEEDS}
      ES_JAVA_OPTS: "-Xmx${ELASTICSEARCH_HEAP} -Xms${ELASTICSEARCH_HEAP} -Des.enforce.bootstrap.checks=true -Dlog4j2.formatMsgNoLookups=true"
      bootstrap.memory_lock: "true"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      - ../elastdocker/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - ../elastdocker/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties
      - ../elastdocker/backup:/backup
    secrets:
      - source: elasticsearch.keystore
        target: /usr/share/elasticsearch/config/elasticsearch.keystore
      - source: elasticsearch.service_tokens
        target: /usr/share/elasticsearch/config/service_tokens
      - source: elastic.ca
        target: /usr/share/elasticsearch/config/certs/ca.crt
      - source: elasticsearch.certificate
        target: /usr/share/elasticsearch/config/certs/elasticsearch.crt
      - source: elasticsearch.key
        target: /usr/share/elasticsearch/config/certs/elasticsearch.key
    ports:
      - ${ELASTICSEARCH_H_PORT}:${ELASTICSEARCH_C_PORT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 200000
        hard: 200000
    healthcheck:
      test:
        [
          "CMD",
          "sh",
          "-c",
          "curl -sf --insecure https://$ELASTIC_USERNAME:$ELASTIC_PASSWORD@localhost:9200/_cat/health | grep -ioE 'green|yellow' || echo 'not green/yellow cluster status'"
        ]
  image_server:
    build: ../image_server
    stdin_open: true
    tty: true
    volumes:
      - ../image_server/src:/src
      - ../image_server/images:/images
    # ports:
    #   - 8888:8880
    command: uvicorn main:api --reload --host 0.0.0.0
    restart: always
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 32gb # set upper limit for how much shared memory container can use

  gateway:
    build: ../gateway
    stdin_open: true
    tty: true
    env_file:
      - .env
    volumes:
      - ../gateway/src:/src
      - ../data:/data
    ports:
      - 8888:8888

  kibana:
    image: elastdocker/kibana:${ELK_VERSION}
    restart: unless-stopped
    volumes:
      - ../elastdocker/kibana/config/:/usr/share/kibana/config:ro
    environment:
      ELASTIC_USERNAME: ${ELASTIC_USERNAME}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTICSEARCH_HOST_PORT: https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_C_PORT}
    env_file:
      - ../elastdocker/secrets/.env.kibana.token
    secrets:
      - source: elastic.ca
        target: /certs/ca.crt
      - source: kibana.certificate
        target: /certs/kibana.crt
      - source: kibana.key
        target: /certs/kibana.key
    ports:
      - ${KIBANA_H_PORT}:${KIBANA_C_PORT}

  weaviate:
    image: semitechnologies/weaviate:${WEAVIATE_VERSION}
    ports:
    - ${WEAVIATE_H_PORT}:${WEAVIATE_C_PORT}
    - ${PROMETHEUS_H_PORT}:${WEAVIATE_C_PORT}
    restart: on-failure:0
    volumes:
      - ../vector_db/var/weaviate:/var/lib/weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      CLUSTER_HOSTNAME: 'node1'
      PROMETHEUS_MONITORING_ENABLED: 'true'

  triton:
    # image: triton
    build: ../triton/
    stdin_open: true
    tty: true
    volumes:
      - ../triton/models:/models
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 16gb # set upper limit for how much shared memory container can use
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    command: tritonserver --model-repository=/models
    
  faceid:
    build: ../visual_inference/mm_faceid/face_id_api
    ports:
      #to the host
      - 8004:8000 #host:container
    stdin_open: true
    tty: true
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 4gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ../visual_inference/mm_faceid/face_id_api/src:/src
    command: uvicorn main:api --reload --host 0.0.0.0

  objdet:
    build: ../visual_inference/mm_objdet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      #to the host
      - 8005:8000 #host:container
    stdin_open: true
    tty: true
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 4gb
    volumes:
      - ../visual_inference/mm_objdet/src:/src
    command: uvicorn main:api --reload --host 0.0.0.0

  vg:
    build: ../visual_inference/mm_vg
    stdin_open: true
    tty: true
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 32gb
    ports:
      - 8006:8000
    deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
    volumes:
      - ../data:/data
      - ../models:/models
      - ../src:/src
    command: uvicorn main:api --reload --host 0.0.0.0
    


  entity-linking-api:
    build: ../text_inference/EL_module
    stdin_open: true
    tty: true
    ports:
      - 5050:5050
    command: ["uvicorn", "api_service:app", "--host", "0.0.0.0", "--port", "5050"]
    volumes:
        - ../text_inference/EL_module/models:/BLINK_api/models
        - ../text_inference/EL_module/configs:/BLINK_api/configs
        - ../text_inference/EL_module/src:/BLINK_api/src
    environment:
      NVIDIA_VISIBLE_DEVICES: 0
      CUDA_VISIBLE_DEVICES: 0
    ulimits:
      memlock: -1
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  NER-api:
    build: ../text_inference/NER_module
    stdin_open: true
    tty: true
    networks:
      - default
    ports:
      - 8080:8080
    command: ["uvicorn", "api_service:app", "--host", "0.0.0.0", "--port", "8080"]
    environment:
      NVIDIA_VISIBLE_DEVICES: 0
      CUDA_VISIBLE_DEVICES: 0
    ulimits:
      memlock: -1
    shm_size: '16gb'
    volumes:
      - ../text_inference/NER_module/spanNER/src:/NER_module/src
      - ../text_inference/spanNER/config:/NER_module/config
      - ../text_inference/spanNER/models:/NER_module/models
      - ../text_inference/data:/NER_module/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  RE-api:
    build: ../text_inference/RE_module
    stdin_open: true
    tty: true
    networks:
      - default
    ports:
      - 8088:8088
    command: ["uvicorn", "api_service:app", "--host", "0.0.0.0", "--port", "8088"]
    environment:
      NVIDIA_VISIBLE_DEVICES: 0
      CUDA_VISIBLE_DEVICES: 0
    ulimits:
      memlock: -1
    shm_size: '16gb'
    volumes:
      - ../text_inference/RE_module/spanREL/src:/RE_module/src
      - ../text_inference/RE_module/spanREL/config:/RE_module/config
      - ../text_inference/RE_module/spanREL/models:/RE_module/models
      - ../text_inference/RE_module/data:/RE_module/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  ui:
    build: ../ui
    stdin_open: true
    tty: true
    # volumes:
    #   - ../document_population/data/images:/images
    environment:
      ELASTIC_USERNAME: ${ELASTIC_USERNAME}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTICSEARCH_HOST_PORT: https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_C_PORT}

    ports:
      - ${UI_H_PORT}:${UI_C_PORT}
